# Audio-Motion Alignment (AMA) æ¨¡å—åˆ›æ–°ç‚¹æ±‡æ€»

> åŸºäº VCT (Vision-Centric Transformer, CVPR 2025) çš„æ”¹è¿›å·¥ä½œ
> 
> ä½œè€…ï¼š[Your Name]
> æ—¥æœŸï¼š2024-12 (æ›´æ–°)

---

## ğŸ“Š å®éªŒç»“æœå¿«é€Ÿæ±‡æ€» (æˆªè‡³ 2024-12-03)

### S4 æ•°æ®é›† (Single-source)
| Method | mIoU | F-score | å¤‡æ³¨ |
|--------|------|---------|------|
| VCT Baseline (ä½œè€…) | 86.20% | 93.40% | è®ºæ–‡æŠ¥å‘Š |
| VCT Baseline (å¤ç°) | 83.78% | 92.16% | æˆ‘ä»¬å¤ç° |
| **VCT + AMA (RAFT, bs4)** | **85.18%** | **92.75%** | â­ **+1.40%** |

### MS3 æ•°æ®é›† (Multi-sources)
| Method | mIoU | F-score | å¤‡æ³¨ |
|--------|------|---------|------|
| VCT Baseline (ä½œè€…) | 66.84% | 82.33% | ä½œè€…æƒé‡ |
| VCT + AMA (bs4) | 65.02% | 78.02% | âŒ è¿‡æ‹Ÿåˆ |
| VCT + AMA + BRM (v1, bs4) | 63.72% | 79.37% | âŒ è¿‡æ‹Ÿåˆæ›´ä¸¥é‡ |
| VCT + AMA + BRM (v2, è®­ç»ƒä¸­) | - | - | ğŸ”„ ä¼˜åŒ–ä¸­ |

### å…³é”®å‘ç°
1. âœ… **S4**: AMA æœ‰æ•ˆï¼Œ+1.40% mIoU (85.18% vs 83.78%)
2. âŒ **MS3**: æ•°æ®å¤ªå°‘ (296 videos)ï¼Œä¸¥é‡è¿‡æ‹Ÿåˆ
3. âš ï¸ **BRM**: è¾¹ç•ŒæŸå¤±ä¸‹é™å¤ªå¿«ï¼ŒåŠ å‰§MS3è¿‡æ‹Ÿåˆ
4. ğŸ’¡ **Batch Size**: 80GB A100ï¼ŒS4/MS3æœ€å¤§bs=4ï¼ŒSSæœ€å¤§bs=2
5. ğŸ“Š **SS**: å¤ç°baseline 49.60%ï¼Œä½äºè®ºæ–‡æŠ¥å‘Š51.20% (-1.6%)

---

### SS æ•°æ®é›† (Semantic Segmentation)
| Method | mIoU | F-score | å¤‡æ³¨ |
|--------|------|---------|------|
| VCT Baseline (è®ºæ–‡æŠ¥å‘Š) | 51.20% | 55.50% | ä½œè€…æŠ¥å‘Š |
| VCT Baseline (å¤ç° best) | 49.60% | 54.00% | æˆ‘ä»¬å¤ç° |
| VCT Baseline (ä»“åº“æƒé‡) | 49.98% | 54.33% | å®˜æ–¹ model_best_ss3 |
| **VCT + AMA + BRM (è®­ç»ƒä¸­)** | - | - | ğŸ”„ è¿›è¡Œä¸­ |

**æ•°æ®é›†çŠ¶æ€**: âœ… å®Œæ•´æ•°æ®é›† (v1m + v1s + v2, 7948 train videos, 71 classes)

---

## 1. ç ”ç©¶åŠ¨æœº (Motivation)

### 1.1 ç°æœ‰æ–¹æ³•çš„é—®é¢˜

VCT ä½¿ç”¨ **PPQG (Prototype Prompted Query Generation)** æ¨¡å—ä»è§†è§‰ç‰¹å¾ä¸­ç”Ÿæˆ Object Queriesã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼š

> **é™æ€è§†è§‰æ˜¾è‘—æ€§é—®é¢˜ (Static Visual Saliency Problem)**ï¼š
> PPQG ä»…ä¾èµ–é™æ€å›¾åƒç‰¹å¾è¿›è¡Œ Query ç”Ÿæˆï¼Œå®¹æ˜“å°†**é™æ€ä½†è§†è§‰æ˜¾è‘—çš„èƒŒæ™¯ç‰©ä½“**ï¼ˆå¦‚è‰²å½©é²œè‰³çš„è£…é¥°å“ã€é«˜å¯¹æ¯”åº¦çš„ç‰©ä½“ï¼‰é”™è¯¯è¯†åˆ«ä¸ºå‘å£°ç‰©ä½“ã€‚

### 1.2 ç®€å•å…‰æµçš„å±€é™æ€§

ä¸€ä¸ªç›´è§‚çš„è§£å†³æ–¹æ¡ˆæ˜¯å¼•å…¥å…‰æµ (Optical Flow) æ¥å…³æ³¨åŠ¨æ€åŒºåŸŸã€‚ä½†ç®€å•å…‰æµå­˜åœ¨å™ªå£°é—®é¢˜ï¼š

> **è¿åŠ¨å™ªå£°é—®é¢˜ (Motion Noise Problem)**ï¼š
> å…‰æµä¼šæ•æ‰æ‰€æœ‰è¿åŠ¨åŒºåŸŸï¼ŒåŒ…æ‹¬**"åŠ¨ä½†æ²¡å£°éŸ³"**çš„ç‰©ä½“ï¼ˆå¦‚é£å¹æ ‘å¶ã€è·¯è¿‡çš„è¡Œäººï¼‰ï¼Œè¿™äº›å™ªå£°ä¼šå¹²æ‰°å‘å£°ç‰©ä½“çš„å®šä½ã€‚

### 1.3 æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆ

**Audio-Motion Alignment (AMA)**ï¼šåˆ©ç”¨éŸ³é¢‘ç‰¹å¾ä½œä¸º Queryï¼Œå…‰æµ/è¿åŠ¨ç‰¹å¾ä½œä¸º Key/Valueï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåªæ¿€æ´»é‚£äº›**"æ—¢åœ¨è¿åŠ¨ã€åˆä¸å£°éŸ³ç›¸å…³"**çš„åŒºåŸŸã€‚

---

## 2. æ–¹æ³•æ¦‚è¿° (Method Overview)

### 2.1 æ•´ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VCT + AMA Framework                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Video   â”‚    â”‚  Audio   â”‚    â”‚     Frame Difference    â”‚    â”‚
â”‚  â”‚  Frames  â”‚    â”‚Spectrogramâ”‚   â”‚      (Motion Map)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚               â”‚                      â”‚                   â”‚
â”‚       â–¼               â–¼                      â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Visual  â”‚    â”‚  Audio   â”‚    â”‚      Flow Encoder       â”‚    â”‚
â”‚  â”‚ Backbone â”‚    â”‚ Encoder  â”‚    â”‚   (Conv â†’ BN â†’ ReLU)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚               â”‚                      â”‚                   â”‚
â”‚       â”‚               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚       â”‚               â”‚    â”‚                                     â”‚
â”‚       â”‚               â–¼    â–¼                                     â”‚
â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚       â”‚         â”‚   AMA Module    â”‚  â† [Ours: Innovation]        â”‚
â”‚       â”‚         â”‚ Q=Audio, K/V=Flowâ”‚                             â”‚
â”‚       â”‚         â”‚ Cross-Attention â”‚                              â”‚
â”‚       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚       â”‚                  â”‚                                       â”‚
â”‚       â”‚                  â–¼                                       â”‚
â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚       â”‚         â”‚ Motion Weight   â”‚                              â”‚
â”‚       â”‚         â”‚ Map (filtered)  â”‚                              â”‚
â”‚       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚       â”‚                  â”‚                                       â”‚
â”‚       â–¼                  â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚      PPQG (ProtoVCQ/VCQ)       â”‚                              â”‚
â”‚  â”‚  feat = feat * (1 + Î» * map)   â”‚  â† Motion-Guided Reweighting â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                    â”‚                                             â”‚
â”‚                    â–¼                                             â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚           â”‚  Transformer    â”‚                                    â”‚
â”‚           â”‚    Decoder      â”‚                                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                    â”‚                                             â”‚
â”‚                    â–¼                                             â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚           â”‚  Segmentation   â”‚                                    â”‚
â”‚           â”‚     Masks       â”‚                                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒåˆ›æ–°ç‚¹

| åˆ›æ–°ç‚¹ | æè¿° | è§£å†³çš„é—®é¢˜ |
|--------|------|-----------|
| **AMA Module** | Audio-Motion Cross-Attention | è¿‡æ»¤"åŠ¨ä½†æ²¡å£°"çš„å™ªå£° |
| **Motion-Guided Reweighting** | ç‰¹å¾åŠ æƒ `x * (1 + Î» * m)` | å¢å¼ºåŠ¨æ€å‘å£°åŒºåŸŸ |
| **Learnable Î»** | å¯å­¦ä¹ çš„è¿åŠ¨æƒé‡å› å­ | è‡ªé€‚åº”è°ƒèŠ‚è¿åŠ¨å½±å“ |

---

## 3. æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### 3.1 Audio-Motion Alignment (AMA) æ¨¡å—

#### 3.1.1 æ¨¡å—å®šä¹‰

**æ–‡ä»¶ä½ç½®**: `models/modeling/transformer_decoder/audio_motion_alignment.py`

```python
class AudioMotionAlignment(nn.Module):
    """
    Audio-Motion Alignment (AMA) Module.
    
    Input:
        audio_feat: (B, C_audio)  - éŸ³é¢‘ç‰¹å¾å‘é‡
        flow_map: (B, 1, H, W)    - å…‰æµ/è¿åŠ¨å›¾
        
    Output:
        motion_weight_map: (B, 1, H, W) - éŸ³é¢‘æ¿€æ´»çš„è¿åŠ¨æƒé‡å›¾ [0,1]
    """
```

#### 3.1.2 ç½‘ç»œç»“æ„

```python
# Flow Encoder: å°†è¿åŠ¨å›¾æ˜ å°„åˆ°åµŒå…¥ç©ºé—´
self.flow_encoder = nn.Sequential(
    nn.Conv2d(flow_channels, 64, kernel_size=3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(inplace=True),
    nn.Conv2d(64, embed_dim, kernel_size=1),
    nn.BatchNorm2d(embed_dim),
)

# Audio Projection: æŠ•å½±éŸ³é¢‘ç‰¹å¾
self.audio_proj = nn.Sequential(
    nn.Linear(audio_dim, embed_dim),
    nn.LayerNorm(embed_dim),
)

# Cross-Attention: Q/K/V projections
self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)  # for Audio
self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)  # for Flow
self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)  # for Flow

# Learnable temperature for attention sharpness
self.temperature = nn.Parameter(torch.ones(1))
```

#### 3.1.3 å‰å‘ä¼ æ’­

```python
def forward(self, audio_feat, flow_map):
    B, C_flow, H, W = flow_map.shape
    
    # 1. Encode flow features
    flow_feat = self.flow_encoder(flow_map)  # (B, embed_dim, H, W)
    flow_feat = flow_feat.flatten(2).permute(0, 2, 1)  # (B, H*W, embed_dim)
    
    # 2. Project audio features as Query
    audio_query = self.audio_proj(audio_feat).unsqueeze(1)  # (B, 1, embed_dim)
    
    # 3. Multi-head Cross-Attention
    Q = self.q_proj(audio_query)  # (B, 1, embed_dim)
    K = self.k_proj(flow_feat)    # (B, H*W, embed_dim)
    V = self.v_proj(flow_feat)    # (B, H*W, embed_dim)
    
    # 4. Scaled dot-product attention with learnable temperature
    scale = (head_dim ** -0.5) * self.temperature
    attn_weights = softmax(Q @ K^T * scale)  # (B, num_heads, 1, H*W)
    
    # 5. Average attention weights across heads â†’ spatial saliency map
    motion_weight_map = attn_weights.mean(dim=1).view(B, 1, H, W)
    
    # 6. Normalize to [0, 1]
    motion_weight_map = (map - min) / (max - min + Îµ)
    
    return motion_weight_map
```

#### 3.1.4 æ•°å­¦å…¬å¼ (ç”¨äºè®ºæ–‡)

**Cross-Attention:**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k} \cdot \tau}\right)V
$$

å…¶ä¸­ï¼š
- $Q = W_q \cdot \text{AudioProj}(f_a)$ï¼Œ$f_a$ æ˜¯éŸ³é¢‘ç‰¹å¾
- $K = W_k \cdot \text{FlowEnc}(m)$ï¼Œ$m$ æ˜¯è¿åŠ¨å›¾
- $V = W_v \cdot \text{FlowEnc}(m)$
- $\tau$ æ˜¯å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°

**Motion Weight Map:**
$$
M_{ama} = \text{Normalize}\left(\frac{1}{H}\sum_{h=1}^{H} A_h\right)
$$

å…¶ä¸­ $A_h$ æ˜¯ç¬¬ $h$ ä¸ªæ³¨æ„åŠ›å¤´çš„æƒé‡ï¼Œ$H$ æ˜¯å¤´æ•°ã€‚

---

### 3.2 Motion-Guided Query Reweighting

#### 3.2.1 ä¿®æ”¹ä½ç½®

**æ–‡ä»¶**: `models/modeling/transformer_decoder/vision_centric_transformer_decoder.py`

**ç±»**: `ProtoVCQ` å’Œ `VCQ`

#### 3.2.2 æ ¸å¿ƒä»£ç 

```python
class ProtoVCQ(nn.Module):
    def __init__(self, ...):
        # ... existing code ...
        
        # [Ours: Motion-Guided] Learnable motion weighting factor Î»
        self.motion_lambda = nn.Parameter(torch.ones(1))

    def forward(self, x, motion_map=None):
        x = self.pre_proj(x)
        
        # [Ours: Motion-Guided] Apply motion-guided reweighting
        if motion_map is not None:
            if motion_map.shape[-2:] != x.shape[-2:]:
                motion_map = F.interpolate(motion_map, size=x.shape[-2:], 
                                           mode='bilinear', align_corners=False)
            # Reweighting: x' = x * (1 + Î» * m)
            x = x * (1.0 + self.motion_lambda * motion_map)
        
        # ... rest of query generation ...
```

#### 3.2.3 æ•°å­¦å…¬å¼

**Feature Reweighting:**
$$
\hat{F}_v = F_v \odot (1 + \lambda \cdot M_{ama})
$$

å…¶ä¸­ï¼š
- $F_v \in \mathbb{R}^{B \times C \times H \times W}$ æ˜¯è§†è§‰ç‰¹å¾
- $M_{ama} \in \mathbb{R}^{B \times 1 \times H \times W}$ æ˜¯ AMA è¾“å‡ºçš„è¿åŠ¨æƒé‡å›¾
- $\lambda$ æ˜¯å¯å­¦ä¹ çš„æ ‡é‡å‚æ•°
- $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆå¹¿æ’­ï¼‰

---

### 3.3 æ•°æ®æµé›†æˆ

#### 3.3.1 Motion Map è®¡ç®— (Dataset Mapper)

**æ–‡ä»¶**: `models/data/dataset_mappers/avss4_semantic_dataset_mapper.py`

```python
# Frame-to-frame motion maps (simplified optical flow)
motion_maps = []
for num_img in range(len(images)):
    if num_img == 0:
        motion_map = torch.zeros((1, H, W), dtype=torch.float32)
    else:
        # L1 difference between consecutive frames
        diff = torch.abs(images[num_img].float() - images[num_img-1].float())
        motion_map = diff.mean(dim=0, keepdim=True)  # Average across RGB
        motion_map = motion_map / motion_map.max()   # Normalize to [0, 1]
    motion_maps.append(motion_map)

dataset_dict["motion_maps"] = torch.stack(motion_maps, dim=0)
```

#### 3.3.2 AMA è°ƒç”¨ (Decoder)

**æ–‡ä»¶**: `models/modeling/transformer_decoder/vision_centric_transformer_decoder.py`

```python
def forward(self, x, audio_features, mask_features, mask=None, motion_maps=None):
    # ...
    
    if motion_maps is not None and len(motion_maps) > 0:
        raw_motion_map = torch.stack(motion_maps, dim=0)  # [bt, 1, H, W]
        
        # Global average pooling for audio features
        audio_feat_for_ama = audio_features.mean(dim=[2, 3])  # [B, 256]
        
        # Downsample motion map for efficiency
        motion_map_small = F.interpolate(raw_motion_map, size=(24, 24), 
                                         mode='bilinear', align_corners=False)
        
        # [Ours: AMA Module] Apply Audio-Motion Alignment
        motion_weight_map = self.ama_module(audio_feat_for_ama, motion_map_small)
        
        # Upsample to mask_features resolution
        motion_weight_map = F.interpolate(motion_weight_map, size=(h_m, w_m), 
                                          mode='bilinear', align_corners=False)
    
    # Pass to PPQG
    visual_querys, prototypes = self.visual_query_block(mask_features, motion_weight_map)
```

---

## 4. ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶è·¯å¾„ | ä¿®æ”¹ç±»å‹ | æè¿° |
|----------|----------|------|
| `models/modeling/transformer_decoder/audio_motion_alignment.py` | **æ–°å»º** | AMA æ¨¡å—å®šä¹‰ |
| `models/modeling/transformer_decoder/vision_centric_transformer_decoder.py` | ä¿®æ”¹ | å¯¼å…¥ AMAï¼Œæ·»åŠ  `self.ama_module`ï¼Œä¿®æ”¹ `forward()` |
| `models/data/dataset_mappers/avss4_semantic_dataset_mapper.py` | ä¿®æ”¹ | æ·»åŠ  `motion_maps` è®¡ç®— |
| `models/data/dataset_mappers/avsms3_semantic_dataset_mapper.py` | ä¿®æ”¹ | æ·»åŠ  `motion_maps` è®¡ç®— |
| `models/vct_model.py` | ä¿®æ”¹ | ä¼ é€’ `motion_maps` åˆ° `sem_seg_head` |
| `models/modeling/meta_arch/vct_model_head.py` | ä¿®æ”¹ | ä¼ é€’ `motion_maps` å‚æ•° |

---

## 5. å®éªŒè®¾ç½®

### 5.1 æ•°æ®é›†

- **S4 (Single-source)**: 4932 videos, 5 frames each
- **MS3 (Multi-sources)**: 424 videos, 5 frames each
- **AVSS (Semantic-labels)**: Full semantic segmentation

### 5.2 è®­ç»ƒé…ç½®

```yaml
SOLVER:
  IMS_PER_BATCH: 4      # é»˜è®¤ batch size (8 ä¼š OOM)
  BASE_LR: 0.00014      # å¯¹åº” batch size 4 çš„å­¦ä¹ ç‡
  MAX_ITER: 20000
  
MODEL:
  MASK_FORMER:
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
```

### 5.3 å­¦ä¹ ç‡ç¼©æ”¾è§„åˆ™ (AdamW ä¼˜åŒ–å™¨)

å¯¹äº **AdamW ä¼˜åŒ–å™¨**ï¼Œå­¦ä¹ ç‡ä¸ batch size å…³ç³»ä½¿ç”¨ **å¹³æ–¹æ ¹ç¼©æ”¾** (Square Root Scaling)ï¼š

$$
lr_{new} = lr_{base} \times \sqrt{\frac{bs_{new}}{bs_{base}}}
$$

| Batch Size | å­¦ä¹ ç‡ (BASE_LR) | è®¡ç®—å…¬å¼ |
|------------|------------------|----------|
| 2 (åŸºå‡†) | 0.0001 | åŸºå‡†å€¼ |
| 4 | 0.00014 | 0.0001 Ã— âˆš2 â‰ˆ 0.00014 |
| **8** | **0.0002** | 0.0001 Ã— âˆš4 = **0.0002** |
| 16 | 0.00028 | 0.0001 Ã— âˆš8 â‰ˆ 0.00028 |

> **æ³¨æ„**: Batch size 8 åœ¨ 80GB A100 ä¸Šä¼š OOMï¼Œå®é™…æœ€å¤§å¯ç”¨ batch size ä¸º 4ã€‚

### 5.3 è¯„ä¼°æŒ‡æ ‡

- **mIoU**: Mean Intersection over Union
- **F-score**: F-measure for binary segmentation

---

## 6. å®éªŒç»“æœ

### 6.1 S4 æ•°æ®é›†ç»“æœ

| Method | mIoU | F-score | ç›¸å¯¹æå‡ | å¤‡æ³¨ |
|--------|------|---------|----------|------|
| VCT (è®ºæ–‡æŠ¥å‘Š) | 86.20% | 93.40% | - | ä½œè€…æŠ¥å‘Š |
| VCT (å¤ç° baseline) | 83.78% | 92.16% | baseline | æˆ‘ä»¬å¤ç° |
| **VCT + AMA v1 (å¸§å·®)** | **84.26%** | **92.40%** | **+0.48%** | Exp-001 |
| **VCT + AMA v2 (RAFT, bs2)** | **84.65%** | **92.39%** | **+0.87%** | Exp-002 |
| **VCT + AMA v3 (RAFT, bs4)** | **85.18%** | **92.75%** | **+1.40%** | Exp-004 â­ |

### 6.2 å®éªŒè¯¦æƒ…è®°å½•

#### Exp-001: AMA + å¸§å·®æ³• (Frame Difference)

**å®éªŒæ—¥æœŸ**: 2024-11-28

**é…ç½®**:
```yaml
# è¿åŠ¨ç‰¹å¾: å¸§å·®æ³• (Frame Difference)
Motion Type: Frame L1 Difference
Motion Channels: 1
AMA embed_dim: 256
AMA num_heads: 4
PPQG Î»: Learnable (init=1.0)

# è®­ç»ƒé…ç½®
SOLVER:
  IMS_PER_BATCH: 8
  BASE_LR: 0.0001
  MAX_ITER: 45000  # S4 å®Œæ•´è®­ç»ƒ
  
# æ•°æ®
Dataset: S4 (4932 videos)
Input Size: 384x384
```

**è®­ç»ƒæ—¥å¿—**: `output/s4_ama_train.log`

**æµ‹è¯•ç»“æœ**:
```
mIoU: 0.8426 (+0.48% vs baseline)
F-score: 0.9240 (+0.24% vs baseline)
```

**åˆ†æ**:
- âœ… æ­£å‘æå‡ï¼Œè¯æ˜ AMA æ€è·¯æœ‰æ•ˆ
- âš ï¸ æå‡å¹…åº¦æœ‰é™ï¼Œå¯èƒ½åŸå› :
  1. å¸§å·®æ³•å™ªå£°å¤§ï¼ˆå…‰ç…§å˜åŒ–ã€ç›¸æœºæŠ–åŠ¨ï¼‰
  2. å¸§å·®åªæ•è· temporal gradientï¼Œç¼ºä¹ spatial motion structure
  3. AMA æ¨¡å—å®¹é‡è¾ƒå°

**ä¸‹ä¸€æ­¥**: ä½¿ç”¨ RAFT é¢„è®¡ç®—é«˜è´¨é‡å…‰æµ

---

#### Exp-002: AMA + RAFT å…‰æµ (RAFT Optical Flow)

**å®éªŒæ—¥æœŸ**: 2024-11-29

**é…ç½®**:
```yaml
# è¿åŠ¨ç‰¹å¾: RAFT å…‰æµ (é¢„è®¡ç®—)
Motion Type: RAFT Optical Flow (magnitude normalized)
Motion Channels: 1
RAFT Model: raft_large (torchvision built-in)
AMA embed_dim: 256
AMA num_heads: 4
PPQG Î»: Learnable (init=1.0)

# è®­ç»ƒé…ç½®
SOLVER:
  IMS_PER_BATCH: 8
  BASE_LR: 0.0001
  MAX_ITER: 45000

# æ•°æ®
Dataset: S4 (4932 videos)
Input Size: 384x384
Flow Storage: /media/a100/.../AVS_dataset/raft_flow/s4/
```

**è®­ç»ƒæ—¥å¿—**: `output/s4_raft_ama_train.log`

**æµ‹è¯•ç»“æœ**:
```
mIoU: 0.8465 (+0.87% vs baseline, +0.39% vs å¸§å·®)
F-score: 0.9239 (+0.23% vs baseline, -0.01% vs å¸§å·®)
```

**åˆ†æ**:
- âœ… RAFT ç›¸æ¯”å¸§å·®æœ‰å¾®å°æå‡ (+0.39% mIoU)
- âš ï¸ ä½†æå‡å¹…åº¦è¿œä½äºé¢„æœŸï¼ˆé¢„æœŸ +1.5%ï¼‰
- âš ï¸ F-score ç”šè‡³ç•¥æœ‰ä¸‹é™

---

#### Exp-004: AMA + RAFT (Batch Size 4, LR 0.00014)

**å®éªŒæ—¥æœŸ**: 2024-12-01

**å®éªŒç›®çš„**: å¢å¤§ batch size æå‡è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦

**é…ç½®**:
```yaml
# è¿åŠ¨ç‰¹å¾: RAFT å…‰æµ (é¢„è®¡ç®—)
Motion Type: RAFT Optical Flow (magnitude normalized)
Motion Channels: 1
AMA embed_dim: 256
AMA num_heads: 4
PPQG Î»: Learnable (init=1.0)

# è®­ç»ƒé…ç½® (è°ƒæ•´ batch size)
SOLVER:
  IMS_PER_BATCH: 4  # å¢å¤§ batch size (åŸ 2)
  BASE_LR: 0.00014  # sqrt(2) ç¼©æ”¾ (åŸ 0.0001)
  MAX_ITER: 45000

# æ•°æ®
Dataset: S4 (4932 videos)
Input Size: 384x384
```

**è®­ç»ƒæ—¥å¿—**: `output/s4_bs8_ama_train.log`

**æµ‹è¯•ç»“æœ**:
```
mIoU: 0.8518 (+1.40% vs baseline 0.8378)
F-score: 0.9275 (+0.59% vs baseline 0.9216)
```

**åˆ†æ**:
- âœ… ç›¸æ¯” batch size 2 æœ‰æ˜¾è‘—æå‡ (+0.53% mIoU)
- âœ… æ€»ä½“ç›¸æ¯” baseline æå‡ +1.40% mIoU
- âš ï¸ ä»ä½äºä½œè€…æŠ¥å‘Šçš„ 86.20%ï¼Œä½†å·®è·åœ¨ç¼©å°
- ğŸ’¡ batch size å½±å“è¾ƒå¤§ï¼Œè¯´æ˜ä¹‹å‰å¯èƒ½è®­ç»ƒä¸å……åˆ†

---

### 6.2 å®éªŒç»“æœæ·±åº¦åˆ†æ

#### 6.2.1 ä¸ºä»€ä¹ˆ AMA æå‡æœ‰é™ï¼Ÿ

| å¯èƒ½åŸå›  | è¯¦ç»†åˆ†æ | éªŒè¯æ–¹æ³• |
|----------|----------|----------|
| **S4 æ•°æ®é›†ç‰¹æ€§** | S4 æ˜¯å•å‘å£°æºåœºæ™¯ï¼Œå‘å£°ç‰©ä½“é€šå¸¸å·²ç»æ˜¯ç”»é¢ä¸­æœ€æ˜¾è‘—çš„è§†è§‰ç›®æ ‡ï¼Œè¿åŠ¨ä¿¡æ¯çš„è¾¹é™…æ”¶ç›Šæœ‰é™ | åœ¨ MS3 å¤šæºåœºæ™¯æµ‹è¯• |
| **è¿åŠ¨ä¸å£°éŸ³å¼±ç›¸å…³** | å¹¶éæ‰€æœ‰å‘å£°ç‰©ä½“éƒ½æœ‰æ˜æ˜¾è¿åŠ¨ï¼ˆå¦‚é™ç½®çš„æ‰¬å£°å™¨ã€æ­£è„¸è¯´è¯çš„äººï¼‰ | å¯è§†åŒ– motion weight map |
| **AMA æ¨¡å—å®¹é‡** | ç®€å•çš„ Cross-Attention å¯èƒ½ä¸è¶³ä»¥å»ºæ¨¡å¤æ‚çš„éŸ³è§†é¢‘å…³ç³» | å¢åŠ æ¨¡å—å¤æ‚åº¦ |
| **å…‰æµè´¨é‡** | RAFT æ˜¯é€šç”¨å…‰æµï¼Œå¯èƒ½å¯¹ AVS ç‰¹å®šåœºæ™¯ä¸å¤Ÿé€‚é… | åˆ†æå¤±è´¥æ¡ˆä¾‹ |
| **è®­ç»ƒä¸å……åˆ†** | AMA æ¨¡å—å‚æ•°è¾ƒå°‘ï¼Œå¯èƒ½è¿‡æ—©æ”¶æ•› | è°ƒæ•´å­¦ä¹ ç‡/è®­ç»ƒè½®æ¬¡ |

#### 6.2.2 Baseline æœ¬èº«ç²¾åº¦é—®é¢˜

æˆ‘ä»¬å¤ç°çš„ baseline (83.78%) æ¯”ä½œè€…æŠ¥å‘Š (86.20%) ä½çº¦ **2.4%**ã€‚è¿™å¯èƒ½å¯¼è‡´:
- æˆ‘ä»¬çš„"æå‡"å®é™…ä¸Šåªæ˜¯å¼¥è¡¥äº† baseline çš„ä¸è¶³
- å¦‚æœ baseline æ­£å¸¸ï¼ŒAMA å¯èƒ½æ ¹æœ¬æ²¡æœ‰æå‡

**å»ºè®®**: å…ˆæ’æŸ¥ baseline ä¸ºä»€ä¹ˆä½äºä½œè€…æŠ¥å‘Š

#### 6.2.3 å•å‘å£°æº vs å¤šå‘å£°æº

| ä»»åŠ¡ | ç‰¹ç‚¹ | AMA é¢„æœŸæ”¶ç›Š |
|------|------|--------------|
| **S4 (å•å‘å£°æº)** | åªæœ‰ä¸€ä¸ªç›®æ ‡ï¼Œé€šå¸¸æœ€æ˜¾è‘— | ä½ |
| **MS3 (å¤šå‘å£°æº)** | å¤šä¸ªç›®æ ‡ï¼Œéœ€è¦åŒºåˆ†è°åœ¨å‘å£° | **é«˜** |
| **AVSS (è¯­ä¹‰)** | æ›´å¤æ‚çš„åœºæ™¯ | ä¸­-é«˜ |

**å…³é”®æ´å¯Ÿ**: AMA çš„è®¾è®¡åˆè¡·æ˜¯**è¿‡æ»¤"åŠ¨ä½†æ²¡å£°"çš„å™ªå£°**ã€‚åœ¨ S4 ä¸­ï¼Œè¿™ç§å™ªå£°å¯èƒ½æœ¬æ¥å°±å¾ˆå°‘ï¼

---

#### Exp-003: MS3 + AMA + RAFT (å¤šå‘å£°æºåœºæ™¯éªŒè¯)

**å®éªŒæ—¥æœŸ**: 2024-12-01 ~ 2024-12-02

**å®éªŒç›®çš„**: éªŒè¯ AMA åœ¨å¤šå‘å£°æºåœºæ™¯ (MS3) ä¸­çš„æ•ˆæœã€‚ç†è®ºä¸Šï¼ŒMS3 æ›´éœ€è¦åŒºåˆ†"è°åœ¨å‘å£°"ï¼ŒAMA åº”è¯¥æ›´æœ‰ä»·å€¼ã€‚

**é…ç½®**:
```yaml
# è¿åŠ¨ç‰¹å¾: RAFT å…‰æµ (é¢„è®¡ç®—)
Motion Type: RAFT Optical Flow (magnitude normalized)
Motion Channels: 1
RAFT Model: raft_large (torchvision built-in)
AMA embed_dim: 256
AMA num_heads: 4
PPQG Î»: Learnable (init=1.0)

# è®­ç»ƒé…ç½® (ä» S4+AMA bs4 æƒé‡åˆå§‹åŒ–)
Initial Weights: S4 + AMA (bs4) model_best.pth
SOLVER:
  IMS_PER_BATCH: 4
  BASE_LR: 0.00014  # sqrt(2) ç¼©æ”¾
  MAX_ITER: 40000

# æ•°æ®
Dataset: MS3 (424 videos: 296 train, 64 val, 64 test)
Input Size: 384x384
Flow Storage: /media/a100/.../AVS_dataset/raft_flow/ms3/
```

**è®­ç»ƒæ—¥å¿—**: `output/ms3_bs4_ama_train.log`

**è®­ç»ƒè¿‡ç¨‹ mIoU å˜åŒ– (éªŒè¯é›†)**:
```
iter 500:   0.6113 â†’ iter 2500: 0.6461 â†’ iter 16500: 0.6708 (å³°å€¼ â­)
â†’ iter 20000: 0.6569 â†’ iter 30000: 0.6440 â†’ iter 40000: 0.6411
```

**æµ‹è¯•ç»“æœ**:
```
éªŒè¯é›† Best: mIoU 0.6708, F-score 0.8354 (iter ~16500)
æµ‹è¯•é›† Final: mIoU 0.6502, F-score 0.7802 âŒ
```

**MS3 Baseline (ä½œè€…åŸå§‹æƒé‡æµ‹è¯•)**:
```
mIoU: 0.6684, F-score 0.8233
```

**é—®é¢˜åˆ†æ**:
| æ•°æ®é›† | AMA (model_best) | Baseline (æ— AMA) | å·®è· |
|--------|------------------|------------------|------|
| éªŒè¯é›† | **0.6708** âœ… | - | +0.24% |
| æµ‹è¯•é›† | 0.6502 âŒ | **0.6684** | **-1.82%** |

**å…³é”®å‘ç°**:
1. âœ… AMA åœ¨**éªŒè¯é›†**ä¸Šè¶…è¿‡äº† baseline (0.6708 > 0.6684)
2. âŒ ä½†åœ¨**æµ‹è¯•é›†**ä¸Šè¡¨ç°ä¸‹é™ (0.6502 < 0.6684)
3. ğŸ”´ **ä¸¥é‡çš„è¿‡æ‹Ÿåˆé—®é¢˜**: æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°éªŒè¯é›†çš„æ¨¡å¼ï¼Œæ³›åŒ–åˆ°æµ‹è¯•é›†å¤±è´¥

**å¯èƒ½åŸå› **:
1. **æ•°æ®é‡å¤ªå°‘**: MS3 åªæœ‰ 296 ä¸ªè®­ç»ƒè§†é¢‘ï¼ŒAMA æ¨¡å—å®¹æ˜“è¿‡æ‹Ÿåˆ
2. **å­¦ä¹ ç‡å¤ªé«˜**: 0.00014 å¯èƒ½å¯¹å°æ•°æ®é›†è¿‡å¤§
3. **æ­£åˆ™åŒ–ä¸è¶³**: AMA æ¨¡å—ç¼ºå°‘ dropout æˆ– weight decay
4. **è®­ç»ƒè¿‡é•¿**: 40000 iter å¯¹ MS3 æ¥è¯´å¯èƒ½å¤ªé•¿
5. **éªŒè¯é›†-æµ‹è¯•é›†åˆ†å¸ƒä¸ä¸€è‡´**: å¯¼è‡´éªŒè¯é›†ä¸Šçš„æœ€ä¼˜ä¸æ˜¯æµ‹è¯•é›†ä¸Šçš„æœ€ä¼˜

---

## 7. è®ºæ–‡å†™ä½œå»ºè®®

### 7.1 æ ‡é¢˜å»ºè®®

- "Audio-Motion Alignment for Robust Audio-Visual Segmentation"
- "Learning to Align Audio and Motion for Sound Source Segmentation"
- "AMA: Filtering Motion Noise with Audio Guidance in AVS"

### 7.2 è´¡çŒ®ç‚¹ (Contributions)

1. **é—®é¢˜å‘ç°**: æŒ‡å‡ºç°æœ‰ AVS æ–¹æ³•å­˜åœ¨çš„é™æ€æ˜¾è‘—æ€§è¯¯æ£€å’Œè¿åŠ¨å™ªå£°é—®é¢˜
2. **æ–¹æ³•åˆ›æ–°**: æå‡º Audio-Motion Alignment (AMA) æ¨¡å—ï¼Œé€šè¿‡éŸ³é¢‘-è¿åŠ¨äº¤å‰æ³¨æ„åŠ›è¿‡æ»¤å™ªå£°
3. **å³æ’å³ç”¨**: AMA æ¨¡å—å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰ AVS æ¡†æ¶ä¸­
4. **å®éªŒéªŒè¯**: åœ¨ AVSBench æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§

### 7.3 Related Work å…³é”®è¯

- Audio-Visual Learning
- Sound Source Localization
- Video Object Segmentation
- Cross-modal Attention
- Optical Flow in Video Understanding

---

## 8. è®­ç»ƒå‘½ä»¤

### S4 æ•°æ®é›†è®­ç»ƒ

```bash
cd /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/VCT_AVS

# å‰å°è¿è¡Œï¼ˆå¯ä»¥çœ‹åˆ°å®æ—¶è¾“å‡ºï¼‰
bash scripts/s4_swinb_384_train.sh

# åå°è¿è¡Œï¼ˆæ–­å¼€ SSH ä¹Ÿèƒ½ç»§ç»­ï¼‰
nohup bash scripts/s4_swinb_384_train.sh > output/s4_ama_train.log 2>&1 &

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f output/s4_ama_train.log
```

### S4 æ•°æ®é›†æµ‹è¯•

```bash
bash scripts/s4_swinb_384_test.sh
```

### MS3 æ•°æ®é›†è®­ç»ƒ

```bash
nohup bash scripts/ms3_swinb_384_train.sh > output/ms3_ama_train.log 2>&1 &
```

---

## 9. ä»£ç æ³¨é‡Šè§„èŒƒ

æ‰€æœ‰ä¿®æ”¹å¤„éƒ½ä½¿ç”¨ä»¥ä¸‹æ³¨é‡Šæ ‡è®°ï¼š

```python
# [Ours: AMA Module] æè¿°
# [Ours: Motion-Guided] æè¿°
```

ä¾¿äºåç»­ä»£ç å®¡æŸ¥å’Œè®ºæ–‡å†™ä½œæ—¶å¿«é€Ÿå®šä½ä¿®æ”¹ä½ç½®ã€‚

---

## 10. åç»­æ”¹è¿›æ–¹å‘

### 10.1 å·²å®Œæˆ
- [x] ~~**çœŸå®å…‰æµ**: ä½¿ç”¨ RAFT ç­‰é¢„è®­ç»ƒå…‰æµæ¨¡å‹æ›¿ä»£å¸§å·®~~ â†’ æ•ˆæœæå‡æœ‰é™

### 10.2 é«˜ä¼˜å…ˆçº§ (æ¨èç«‹å³å°è¯•)

#### æ–¹æ¡ˆ A: è½¬æˆ˜ MS3 æ•°æ®é›† â­â­â­â­â­
```
ç†ç”±: 
- MS3 æ˜¯å¤šå‘å£°æºåœºæ™¯ï¼Œæ›´éœ€è¦åŒºåˆ†"è°åœ¨å‘å£°"
- AMA çš„æ ¸å¿ƒä»·å€¼ (è¿‡æ»¤è¿åŠ¨å™ªå£°) åœ¨ MS3 ä¸­æ›´èƒ½ä½“ç°
- S4 æå‡æœ‰é™å¯èƒ½æ˜¯æ•°æ®é›†ç‰¹æ€§å†³å®šçš„

æ“ä½œ:
1. å‡†å¤‡ MS3 æ•°æ®é›† (å·²æœ‰)
2. è®¡ç®— MS3 çš„ RAFT å…‰æµ
3. è®­ç»ƒ VCT + AMA on MS3
4. å¯¹æ¯” baseline
```

#### æ–¹æ¡ˆ B: å¯è§†åŒ–åˆ†æ + å¤±è´¥æ¡ˆä¾‹ â­â­â­â­
```
ç†ç”±:
- ä¸ç›²ç›®æ”¹è¿›ï¼Œå…ˆç†è§£ç°æœ‰æ¨¡å—çš„è¡Œä¸º
- æ‰¾åˆ° AMA æ²¡èµ·ä½œç”¨çš„å…·ä½“åŸå› 

æ“ä½œ:
1. å¯è§†åŒ– motion_weight_map (AMA è¾“å‡º)
2. å¯¹æ¯” AMA vs æ—  AMA çš„é¢„æµ‹å·®å¼‚
3. åˆ†æ: å“ªäº›æ¡ˆä¾‹ AMA å¸®åŠ©äº†ï¼Ÿå“ªäº›æ²¡å¸®åŠ©ï¼Ÿ
```

#### æ–¹æ¡ˆ C: ä¿®æ­£ Baseline â­â­â­â­
```
ç†ç”±:
- æˆ‘ä»¬ baseline 83.78% æ¯”ä½œè€… 86.20% ä½ 2.4%
- å¯èƒ½æœ‰é…ç½®/æ•°æ®å¤„ç†é—®é¢˜
- åœ¨æ­£ç¡®çš„ baseline ä¸Šæµ‹ AMA æ›´æœ‰æ„ä¹‰

æ“ä½œ:
1. ä»”ç»†å¯¹æ¯”ä½œè€…é…ç½®
2. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦ä¸€è‡´
3. å°è¯•ä¸åŒéšæœºç§å­
```

### 10.3 ä¸­ä¼˜å…ˆçº§

4. **Audio-Motion Consistency Loss**: æ·»åŠ è¾…åŠ©æŸå¤±çº¦æŸéŸ³é¢‘-è¿åŠ¨å¯¹é½
5. **å¤šå°ºåº¦ AMA**: åœ¨ Pixel Decoder çš„å¤šä¸ªå°ºåº¦åº”ç”¨ AMA
6. **æ—¶åºå»ºæ¨¡**: ä½¿ç”¨ LSTM/Transformer å»ºæ¨¡æ—¶åºè¿åŠ¨ä¿¡æ¯
7. **AMA æ¶æ„æ”¹è¿›**: å°è¯•æ›´å¤æ‚çš„èåˆç­–ç•¥ (å¦‚ Gated Fusionã€Cross-Modal Transformer)

### 10.4 å®éªŒè·¯çº¿å›¾ (å»ºè®®)

```
Week 1: [å½“å‰] S4 + AMA (å¸§å·®/RAFT) â†’ æ•ˆæœæœ‰é™ âœ“
        
Week 2: MS3 + AMA â†’ éªŒè¯å¤šæºåœºæ™¯æ”¶ç›Š
        å¯è§†åŒ–åˆ†æ â†’ ç†è§£ AMA è¡Œä¸º
        
Week 3: æ ¹æ® Week 2 ç»“æœé€‰æ‹©æ–¹å‘:
        - å¦‚æœ MS3 æå‡å¤§ â†’ å®Œå–„æ–¹æ³•ï¼Œå‡†å¤‡æŠ•ç¨¿
        - å¦‚æœ MS3 ä¹Ÿä¸è¡Œ â†’ æ¢åˆ›æ–°ç‚¹æˆ–æ·±å…¥åˆ†æåŸå› 
```

---

## 11. RAFT å…‰æµé¢„è®¡ç®—æ–¹æ¡ˆ (è¯¦ç»†)

### 11.1 ä¸ºä»€ä¹ˆéœ€è¦ RAFTï¼Ÿ

| å¯¹æ¯”é¡¹ | å¸§å·®æ³• (Frame Diff) | RAFT å…‰æµ |
|--------|---------------------|-----------|
| **ç²¾åº¦** | ä½ï¼Œåªæœ‰ temporal gradient | é«˜ï¼ŒSOTA å…‰æµç®—æ³• |
| **å™ªå£°** | å¯¹å…‰ç…§ã€ç›¸æœºæŠ–åŠ¨æ•æ„Ÿ | é²æ£’ï¼Œç»è¿‡å¤§è§„æ¨¡è®­ç»ƒ |
| **è¯­ä¹‰** | æ— ï¼Œçº¯åƒç´ çº§å·®å¼‚ | æœ‰ï¼Œæ•è·çœŸå®è¿åŠ¨ç»“æ„ |
| **è®¡ç®—é‡** | å‡ ä¹ä¸º0 | é«˜ï¼Œä½†å¯é¢„è®¡ç®— |
| **è¾“å‡ºç»´åº¦** | 1 é€šé“ (magnitude) | 2 é€šé“ (u, v) æˆ– magnitude |

### 11.2 RAFT ç®€ä»‹

**RAFT (Recurrent All-Pairs Field Transforms)** - ECCV 2020 Best Paper

- **è®ºæ–‡**: "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"
- **ä»£ç **: https://github.com/princeton-vl/RAFT
- **ç‰¹ç‚¹**:
  - åœ¨ Sintelã€KITTI ç­‰å…‰æµæ•°æ®é›†ä¸Š SOTA
  - è¿­ä»£å¼ refinementï¼Œç²¾åº¦é«˜
  - æœ‰å¤šç§é¢„è®­ç»ƒæ¨¡å‹å¯é€‰

### 11.3 å®æ–½æ–¹æ¡ˆ

#### Step 1: å®‰è£… RAFT

```bash
# å…‹éš† RAFT ä»“åº“
cd /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj
git clone https://github.com/princeton-vl/RAFT.git
cd RAFT

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
./download_models.sh
# æˆ–æ‰‹åŠ¨ä¸‹è½½: https://drive.google.com/drive/folders/1sWDsfuZ3Up38EUQt7-JDTT1HcGHuJgvT

# é¢„è®­ç»ƒæ¨¡å‹è¯´æ˜:
# - raft-things.pth: åœ¨ FlyingThings3D ä¸Šè®­ç»ƒï¼Œæ³›åŒ–æ€§å¥½
# - raft-sintel.pth: åœ¨ Sintel ä¸Šå¾®è°ƒï¼Œé€‚åˆçœŸå®åœºæ™¯
# - raft-kitti.pth: åœ¨ KITTI ä¸Šå¾®è°ƒï¼Œé€‚åˆé©¾é©¶åœºæ™¯
# æ¨èä½¿ç”¨ raft-things.pth æˆ– raft-sintel.pth
```

#### Step 2: é¢„è®¡ç®—å…‰æµè„šæœ¬

åˆ›å»º `avs_tools/compute_raft_flow.py`:

```python
"""
[Ours: RAFT Flow] é¢„è®¡ç®— RAFT å…‰æµ
Usage:
    python avs_tools/compute_raft_flow.py \
        --dataset s4 \
        --input_root /path/to/AVS_dataset \
        --output_root /path/to/AVS_dataset/raft_flow \
        --model /path/to/RAFT/models/raft-sintel.pth
"""

import os
import sys
import argparse
import numpy as np
import torch
from PIL import Image
from tqdm import tqdm

# æ·»åŠ  RAFT è·¯å¾„
sys.path.append('/media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/RAFT/core')
from raft import RAFT
from utils.utils import InputPadder


def load_image(path):
    """åŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸º tensor"""
    img = np.array(Image.open(path)).astype(np.uint8)
    img = torch.from_numpy(img).permute(2, 0, 1).float()
    return img[None].cuda()


def compute_flow(model, image1, image2):
    """è®¡ç®—å…‰æµ"""
    padder = InputPadder(image1.shape)
    image1, image2 = padder.pad(image1, image2)
    
    with torch.no_grad():
        _, flow = model(image1, image2, iters=20, test_mode=True)
    
    flow = padder.unpad(flow)
    return flow[0].cpu().numpy()  # [2, H, W]


def flow_to_magnitude(flow):
    """å°†å…‰æµè½¬æ¢ä¸º magnitude (å•é€šé“)"""
    u, v = flow[0], flow[1]
    magnitude = np.sqrt(u**2 + v**2)
    # å½’ä¸€åŒ–åˆ° [0, 1]
    magnitude = magnitude / (magnitude.max() + 1e-6)
    return magnitude.astype(np.float32)


def main(args):
    # åŠ è½½ RAFT æ¨¡å‹
    model = torch.nn.DataParallel(RAFT(args))
    model.load_state_dict(torch.load(args.model))
    model = model.module.cuda().eval()
    
    # æ•°æ®é›†è·¯å¾„é…ç½®
    if args.dataset == 's4':
        splits = ['train', 'val', 'test']
        frame_dir = 'visual_frames_384'
    elif args.dataset == 'ms3':
        splits = ['train', 'val', 'test']
        frame_dir = 'visual_frames_384'
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}")
    
    for split in splits:
        input_path = os.path.join(args.input_root, f'{args.dataset}_data_384', frame_dir, split)
        output_path = os.path.join(args.output_root, args.dataset, split)
        os.makedirs(output_path, exist_ok=True)
        
        videos = sorted(os.listdir(input_path))
        print(f"Processing {split}: {len(videos)} videos")
        
        for video in tqdm(videos, desc=f'{args.dataset}/{split}'):
            video_input = os.path.join(input_path, video)
            video_output = os.path.join(output_path, video)
            os.makedirs(video_output, exist_ok=True)
            
            frames = sorted([f for f in os.listdir(video_input) if f.endswith('.png')])
            
            for i in range(len(frames)):
                output_file = os.path.join(video_output, f'flow_{i:04d}.npy')
                
                if os.path.exists(output_file):
                    continue  # è·³è¿‡å·²è®¡ç®—çš„
                
                if i == 0:
                    # ç¬¬ä¸€å¸§ï¼šä½¿ç”¨é›¶å…‰æµ
                    img = Image.open(os.path.join(video_input, frames[0]))
                    h, w = img.size[1], img.size[0]
                    flow_mag = np.zeros((h, w), dtype=np.float32)
                else:
                    # è®¡ç®— frame[i-1] -> frame[i] çš„å…‰æµ
                    img1 = load_image(os.path.join(video_input, frames[i-1]))
                    img2 = load_image(os.path.join(video_input, frames[i]))
                    flow = compute_flow(model, img1, img2)
                    flow_mag = flow_to_magnitude(flow)
                
                np.save(output_file, flow_mag)
    
    print("Done!")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, required=True, choices=['s4', 'ms3', 'avss'])
    parser.add_argument('--input_root', type=str, required=True)
    parser.add_argument('--output_root', type=str, required=True)
    parser.add_argument('--model', type=str, required=True)
    
    # RAFT æ¨¡å‹å‚æ•° (ä¿æŒé»˜è®¤)
    parser.add_argument('--small', action='store_true', help='use small model')
    parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')
    
    args = parser.parse_args()
    main(args)
```

#### Step 3: è¿è¡Œé¢„è®¡ç®—

```bash
# S4 æ•°æ®é›†
python avs_tools/compute_raft_flow.py \
    --dataset s4 \
    --input_root /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/AVS_dataset/AVSBench_object/Single-source \
    --output_root /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/AVS_dataset/raft_flow \
    --model /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/RAFT/models/raft-sintel.pth

# MS3 æ•°æ®é›†
python avs_tools/compute_raft_flow.py \
    --dataset ms3 \
    --input_root /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/AVS_dataset/AVSBench_object/Multi-sources \
    --output_root /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/AVS_dataset/raft_flow \
    --model /media/a100/c5e1bf65-7974-432f-8aed-7a1345241efe/hhj/RAFT/models/raft-sintel.pth
```

#### Step 4: ä¿®æ”¹ DatasetMapper

åœ¨ `avss4_semantic_dataset_mapper.py` ä¸­æ·»åŠ è¯»å–é¢„è®¡ç®—å…‰æµçš„é€»è¾‘:

```python
# [Ours: RAFT Flow] è¯»å–é¢„è®¡ç®—çš„ RAFT å…‰æµ
def load_raft_flow(self, video_name, frame_idx, split):
    """åŠ è½½é¢„è®¡ç®—çš„ RAFT å…‰æµ"""
    flow_root = "/media/a100/.../AVS_dataset/raft_flow"
    flow_path = os.path.join(flow_root, 's4', split, video_name, f'flow_{frame_idx:04d}.npy')
    
    if os.path.exists(flow_path):
        flow_mag = np.load(flow_path)  # [H, W], float32, [0, 1]
        return torch.from_numpy(flow_mag).unsqueeze(0)  # [1, H, W]
    else:
        # Fallback: å¸§å·®æ³•
        return None

# åœ¨ __call__ ä¸­ä½¿ç”¨
motion_maps = []
for num_img in range(len(images)):
    flow = self.load_raft_flow(video_name, num_img, split)
    if flow is None:
        # Fallback to frame difference
        if num_img == 0:
            motion_map = torch.zeros((1, H, W), dtype=torch.float32)
        else:
            diff = torch.abs(images[num_img].float() - images[num_img-1].float())
            motion_map = diff.mean(dim=0, keepdim=True)
            motion_map = motion_map / (motion_map.max() + 1e-6)
    else:
        motion_map = flow
    motion_maps.append(motion_map)
```

### 11.4 é¢„æœŸæ•ˆæœ

| æ–¹æ³• | é¢„æœŸ mIoU | åŸå›  |
|------|-----------|------|
| å¸§å·® + AMA | 84.26% | å½“å‰ç»“æœ |
| **RAFT + AMA** | **85.5-86.5%** | é«˜è´¨é‡è¿åŠ¨ç‰¹å¾ |

### 11.5 å­˜å‚¨ç©ºé—´ä¼°ç®—

```
S4 æ•°æ®é›†:
- 4932 videos Ã— 5 frames Ã— 384Ã—384 Ã— 4 bytes (float32)
â‰ˆ 14.5 GB

MS3 æ•°æ®é›†:
- 424 videos Ã— 5 frames Ã— 384Ã—384 Ã— 4 bytes
â‰ˆ 1.2 GB

æ€»è®¡: ~16 GB
```

### 11.6 è®¡ç®—æ—¶é—´ä¼°ç®—

```
RAFT æ¨ç†é€Ÿåº¦ (RTX 3090/A100):
- ~0.1-0.2 ç§’/å¸§ (384Ã—384)

S4: 4932 Ã— 5 = 24660 å¸§ Ã— 0.15 ç§’ â‰ˆ 1 å°æ—¶
MS3: 424 Ã— 5 = 2120 å¸§ Ã— 0.15 ç§’ â‰ˆ 5 åˆ†é’Ÿ
```

---

## 12. TensorBoard ç›‘æ§

TensorBoard å·²é…ç½®ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®:

```bash
# å¯åŠ¨ TensorBoard (å·²åœ¨åå°è¿è¡Œ)
tensorboard --logdir=output --port=6006 --bind_all

# æœ¬åœ°è®¿é—®
http://localhost:6006

# è¿œç¨‹ SSH ç«¯å£è½¬å‘
ssh -L 6006:localhost:6006 user@server_ip
```

ç›‘æ§æŒ‡æ ‡:
- `total_loss`: æ€»æŸå¤±
- `loss_ce`: äº¤å‰ç†µæŸå¤±
- `loss_dice`: Dice æŸå¤±
- `lr`: å­¦ä¹ ç‡

---

*æ–‡æ¡£æ›´æ–°æ—¶é—´: 2024-11-28*

